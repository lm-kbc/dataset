{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Large-scale language models (LMs) such as BERT are optimized to predict masked-out textual inputs and have notably advanced performances on a range of downstream NLP tasks. Recently, LMs also gained attention for their purported ability to yield structured pieces of knowledge directly from their parameters. This is promising as current knowledge bases (KBs) such as Wikidata and ConceptNet are part of the backbone of the Semantic Web ecosystem, yet are inherently incomplete. In the recent seminal LAMA paper [(Petroni et al., 2019)](https://arxiv.org/pdf/1909.01066.pdf), authors showed that LMs could highly rank correct object tokens when given an input prompt specifying the subject-entity and relation. Despite much follow-up work reporting further advancements, the prospect of using LMs for knowledge base construction remains unexplored. \n",
    "\n",
    "We invite participants to present solutions to make use of **LMs for KB construction** without prior information on the cardinality of relations, i.e., for a given subject-relation pair, the details on the total count of possible object-entities are absent. We require participants to submit a system that takes an input consisting of a subject-entity and relation, uses an LM depending on the choice of the track (BERT-type or open), generates subject-relation-object tuples, and makes actual accept/reject decisions for each generated output triple. Finally, we evaluate the resulting KBs using established F1-score (harmonic mean of precision and recall) metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE:** Before continuing further, follow the steps given in [README.md](https://github.com/lm-kbc/dataset/blob/main/README.md) to install the required python packages if you haven't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LM Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Knowledge Base Construction from Language Models (LM-KBC) pipeline has the following important modules:\n",
    "\n",
    "1. Choosing the subject-entity (e.g., Germany) and relation (e.g., CountryBordersWithCountry)\n",
    "2. Creating a prompt ( e.g., \"_Germany shares border with [MASK]_.\", a masked prompt for BERT-type masked language models)\n",
    "3. Probing an existing language model using the above prompt as an input\n",
    "4. Obtaining LM's output, which are the likelihood based ranked object-entities in the [MASK] position, using the  on the input prompt\n",
    "5. Applying a selection criteria on LM's output to get only the factually correct object-entitites for the given subject-entity and relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>Participants can propose solutions that either improves the performance of these modules compared to the given baseline system or submit a new idea to better generate the object-entities, with the goal to beat the baseline's F1-score. Below we explain how some of these modules affect the LM's output when probed.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, logging\n",
    "\n",
    "from baseline import create_prompt  # our baseline's prompt templates\n",
    "from file_io import read_lm_kbc_jsonl_to_df  # function to read the ground-truth files\n",
    "\n",
    "logging.set_verbosity_error()  # avoid irritating transformers warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our probing function\n",
    "def probe_lm(model_name, subject_entity, relation, top_k=100, prompt=None):\n",
    "    # Load the model\n",
    "    print(f\"Loading model \\\"{model_name}\\\"...\", end=\" \")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        task=\"fill-mask\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    \n",
    "    # Create prompt\n",
    "    if not prompt:\n",
    "        prompt = create_prompt(subject_entity, relation, mask_token)\n",
    "    \n",
    "    # Probe the LM\n",
    "    print(\"Probing...\")\n",
    "    outputs = pipe(prompt)\n",
    "    \n",
    "    return [{\n",
    "        \"Prompt\": prompt,\n",
    "        \"SubjectEntity\": subject_entity,\n",
    "        \"Relation\": relation,\n",
    "        \"ObjectEntity\": out[\"token_str\"],\n",
    "        \"Probability\": out[\"score\"]\n",
    "    } for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Assume the following subject entity and relation from here on\n",
    "subject_entity = \"Singapore\"\n",
    "relation = \"CountryBordersWithCountry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Effect of Languge Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how the output object-entities varies for three different pre-trained LMs: \n",
    "- BERT-base-cased\n",
    "- BERT-large-cased\n",
    "- RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model \"bert-base-cased\"... Probing...\n",
      "Loading model \"bert-large-cased\"... Probing...\n",
      "Loading model \"roberta-base\"... Probing...\n"
     ]
    }
   ],
   "source": [
    "# probing the three different LMs on the chosen subject-entity and relation\n",
    "bert_base_cased_output = probe_lm(\"bert-base-cased\", subject_entity, relation)\n",
    "bert_large_cased_output = probe_lm(\"bert-large-cased\", subject_entity, relation)\n",
    "roberta_base_output = probe_lm(\"roberta-base\", subject_entity, relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the probability threshold equal to 0.5 (our selection criteria) and filtering the LMs outputs by this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob_threshold = 0.5\n",
    "\n",
    "filtered_bert_base_cased_output = [\n",
    "    out[\"ObjectEntity\"] for out in bert_base_cased_output if out[\"Probability\"] >= prob_threshold\n",
    "]\n",
    "\n",
    "filtered_bert_large_cased_output = [\n",
    "    out[\"ObjectEntity\"] for out in bert_large_cased_output if out[\"Probability\"] >= prob_threshold\n",
    "]\n",
    "\n",
    "filtered_roberta_base_output = [\n",
    "    out[\"ObjectEntity\"] for out in roberta_base_output if out[\"Probability\"] >= prob_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth object-entities are: [[['malaysia'], ['indonesia']]]\n",
      "bert_base_output: []\n",
      "bert_large_output: ['Malaysia']\n",
      "roberta_base_output: []\n"
     ]
    }
   ],
   "source": [
    "# retrieving the ground truth labels from the given train dataset for the chosen subject-entity and relation\n",
    "df = read_lm_kbc_jsonl_to_df(\"data/train.jsonl\")\n",
    "ground_truth = df[(df[\"SubjectEntity\"] == subject_entity) & (df[\"Relation\"] == relation)][\"ObjectEntities\"].tolist()\n",
    "print(\"Ground truth object-entities are:\", ground_truth)\n",
    "\n",
    "# printing the filtered outputs\n",
    "print(\"bert_base_output:\", filtered_bert_base_cased_output)\n",
    "print(\"bert_large_output:\", filtered_bert_large_cased_output)\n",
    "print(\"roberta_base_output:\", filtered_roberta_base_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that the choice of the pre-trained language model has a direct effect on the generated output. Participants can try to further fine-tune the BERT model (for track 1) on this task or experiment with other existing pre-training LMs (for track 2).<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Effect of prompt formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how the output object-entities varies while using different prompt structures on the BERT-large-cased LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating different prompts:\n",
    "\n",
    "bert_large_masked_token = \"[MASK]\"\n",
    "\n",
    "prompt0 = f\"{subject_entity} shares border with {bert_large_masked_token}.\"\n",
    "prompt1 = f\"{subject_entity} borders {bert_large_masked_token}\"\n",
    "prompt2 = f\"{subject_entity} borders {bert_large_masked_token}, which is a country\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model \"bert-large-cased\"... Probing...\n",
      "Loading model \"bert-large-cased\"... Probing...\n",
      "Loading model \"bert-large-cased\"... Probing...\n"
     ]
    }
   ],
   "source": [
    "# probing the BERT-large-cased LM using the three different prompts for same subject-entity and relation\n",
    "prompt0_output = probe_lm(\"bert-large-cased\", subject_entity, relation, prompt=prompt0)\n",
    "prompt1_output = probe_lm(\"bert-large-cased\", subject_entity, relation, prompt=prompt1)\n",
    "prompt2_output = probe_lm(\"bert-large-cased\", subject_entity, relation, prompt=prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top-3 results of each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>SubjectEntity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>ObjectEntity</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore shares border with [MASK].</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.690764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore shares border with [MASK].</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>0.112451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singapore shares border with [MASK].</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>0.067123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Prompt SubjectEntity  \\\n",
       "0  Singapore shares border with [MASK].     Singapore   \n",
       "1  Singapore shares border with [MASK].     Singapore   \n",
       "2  Singapore shares border with [MASK].     Singapore   \n",
       "\n",
       "                    Relation ObjectEntity  Probability  \n",
       "0  CountryBordersWithCountry     Malaysia     0.690764  \n",
       "1  CountryBordersWithCountry     Thailand     0.112451  \n",
       "2  CountryBordersWithCountry    Indonesia     0.067123  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prompt0_output).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>SubjectEntity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>ObjectEntity</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore borders [MASK]</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>;</td>\n",
       "      <td>0.517871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore borders [MASK]</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>.</td>\n",
       "      <td>0.481356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singapore borders [MASK]</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>|</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Prompt SubjectEntity                   Relation  \\\n",
       "0  Singapore borders [MASK]     Singapore  CountryBordersWithCountry   \n",
       "1  Singapore borders [MASK]     Singapore  CountryBordersWithCountry   \n",
       "2  Singapore borders [MASK]     Singapore  CountryBordersWithCountry   \n",
       "\n",
       "  ObjectEntity  Probability  \n",
       "0            ;     0.517871  \n",
       "1            .     0.481356  \n",
       "2            |     0.000378  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prompt1_output).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>SubjectEntity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>ObjectEntity</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore borders [MASK], which is a country</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.382691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore borders [MASK], which is a country</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>0.146750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singapore borders [MASK], which is a country</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>0.094743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Prompt SubjectEntity  \\\n",
       "0  Singapore borders [MASK], which is a country     Singapore   \n",
       "1  Singapore borders [MASK], which is a country     Singapore   \n",
       "2  Singapore borders [MASK], which is a country     Singapore   \n",
       "\n",
       "                    Relation ObjectEntity  Probability  \n",
       "0  CountryBordersWithCountry     Malaysia     0.382691  \n",
       "1  CountryBordersWithCountry    Indonesia     0.146750  \n",
       "2  CountryBordersWithCountry     Thailand     0.094743  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prompt2_output).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that the prompt used for probing affects the quality of the generated output. Participants can propose a solution that automatically designs better and optimal prompts for this task.<font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Effect of selection criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how the choosing different the probability thresholds affects the generated output object-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initializing different probability thresholds\n",
    "prob_threshold1 = 0.1\n",
    "prob_threshold2 = 0.5\n",
    "prob_threshold3 = 0.9\n",
    "\n",
    "# filtering bert-large outputs using the thresholds\n",
    "thres1_result = [out for out in bert_large_cased_output if out[\"Probability\"] >= prob_threshold1]\n",
    "thres2_result = [out for out in bert_large_cased_output if out[\"Probability\"] >= prob_threshold2]\n",
    "thres3_result = [out for out in bert_large_cased_output if out[\"Probability\"] >= prob_threshold3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top-3 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>SubjectEntity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>ObjectEntity</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore shares border with [MASK].</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.690764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore shares border with [MASK].</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>0.112451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Prompt SubjectEntity  \\\n",
       "0  Singapore shares border with [MASK].     Singapore   \n",
       "1  Singapore shares border with [MASK].     Singapore   \n",
       "\n",
       "                    Relation ObjectEntity  Probability  \n",
       "0  CountryBordersWithCountry     Malaysia     0.690764  \n",
       "1  CountryBordersWithCountry     Thailand     0.112451  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(thres1_result).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>SubjectEntity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>ObjectEntity</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore shares border with [MASK].</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CountryBordersWithCountry</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.690764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Prompt SubjectEntity  \\\n",
       "0  Singapore shares border with [MASK].     Singapore   \n",
       "\n",
       "                    Relation ObjectEntity  Probability  \n",
       "0  CountryBordersWithCountry     Malaysia     0.690764  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(thres2_result).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(thres3_result).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that changing the threshold leads to very different performance scores. When the threshold is 0.1, F1-score would be 0.5 (1 out of 2 generations is correct and 1 out of the 2 ground truth object-entities was selected); however for threshold 0.9, F1-score would be 0. Participants can propose a solution that uses a better thresholding mechanism or even further calibrate the LM's likelihood on this task.<font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
